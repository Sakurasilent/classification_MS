{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试bert使用是否正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./hgmodel/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertModel\n",
    "from config import *\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./hgmodel/bert-base-chinese/\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"./hgmodel/bert-base-chinese/\")\n",
    "# model = BertModel.from_pretrained(\"./hgmodel/bert-base-chinese/\")\n",
    "model = BertModel.from_pretrained(BERT_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from config import *\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./hgmodel/bert-base-chinese/ were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 0 batch: 0 loss: 2.35447 train_acc: 0.11 dev_acc: 0.22\n",
      ">> epoch: 0 batch: 50 loss: 0.55928 train_acc: 0.81 dev_acc: 0.84\n",
      ">> epoch: 0 batch: 100 loss: 0.25447 train_acc: 0.92 dev_acc: 0.86\n",
      ">> epoch: 0 batch: 150 loss: 0.23782 train_acc: 0.93 dev_acc: 0.84\n",
      ">> epoch: 0 batch: 200 loss: 0.37908 train_acc: 0.86 dev_acc: 0.88\n",
      ">> epoch: 0 batch: 250 loss: 0.1411 train_acc: 0.96 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 300 loss: 0.33082 train_acc: 0.89 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 350 loss: 0.1376 train_acc: 0.97 dev_acc: 0.87\n",
      ">> epoch: 0 batch: 400 loss: 0.23227 train_acc: 0.93 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 450 loss: 0.22827 train_acc: 0.91 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 500 loss: 0.22975 train_acc: 0.94 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 550 loss: 0.27621 train_acc: 0.92 dev_acc: 0.92\n",
      ">> epoch: 0 batch: 600 loss: 0.30631 train_acc: 0.91 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 650 loss: 0.42344 train_acc: 0.88 dev_acc: 0.86\n",
      ">> epoch: 0 batch: 700 loss: 0.45664 train_acc: 0.87 dev_acc: 0.92\n",
      ">> epoch: 0 batch: 750 loss: 0.22202 train_acc: 0.94 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 800 loss: 0.18724 train_acc: 0.95 dev_acc: 0.92\n",
      ">> epoch: 0 batch: 850 loss: 0.22031 train_acc: 0.92 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 900 loss: 0.48394 train_acc: 0.84 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 950 loss: 0.24404 train_acc: 0.92 dev_acc: 0.87\n",
      ">> epoch: 0 batch: 1000 loss: 0.43669 train_acc: 0.88 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 1050 loss: 0.33701 train_acc: 0.88 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 1100 loss: 0.22278 train_acc: 0.93 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 1150 loss: 0.1337 train_acc: 0.95 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 1200 loss: 0.12604 train_acc: 0.96 dev_acc: 0.94\n",
      ">> epoch: 0 batch: 1250 loss: 0.23202 train_acc: 0.9 dev_acc: 0.88\n",
      ">> epoch: 0 batch: 1300 loss: 0.28329 train_acc: 0.93 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 1350 loss: 0.31096 train_acc: 0.91 dev_acc: 0.89\n",
      ">> epoch: 0 batch: 1400 loss: 0.28976 train_acc: 0.95 dev_acc: 0.93\n",
      ">> epoch: 0 batch: 1450 loss: 0.24267 train_acc: 0.9 dev_acc: 0.92\n",
      ">> epoch: 0 batch: 1500 loss: 0.35968 train_acc: 0.93 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 1550 loss: 0.14899 train_acc: 0.96 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 1600 loss: 0.26275 train_acc: 0.91 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 1650 loss: 0.16935 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 0 batch: 1700 loss: 0.21414 train_acc: 0.94 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 1750 loss: 0.13125 train_acc: 0.95 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 0 loss: 0.14685 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 50 loss: 0.30911 train_acc: 0.87 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 100 loss: 0.21905 train_acc: 0.92 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 150 loss: 0.14817 train_acc: 0.95 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 200 loss: 0.25493 train_acc: 0.93 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 250 loss: 0.16283 train_acc: 0.97 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 300 loss: 0.16148 train_acc: 0.96 dev_acc: 0.89\n",
      ">> epoch: 1 batch: 350 loss: 0.15684 train_acc: 0.94 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 400 loss: 0.16426 train_acc: 0.95 dev_acc: 0.89\n",
      ">> epoch: 1 batch: 450 loss: 0.12722 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 500 loss: 0.17147 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 550 loss: 0.19768 train_acc: 0.93 dev_acc: 0.89\n",
      ">> epoch: 1 batch: 600 loss: 0.24386 train_acc: 0.91 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 650 loss: 0.17965 train_acc: 0.93 dev_acc: 0.88\n",
      ">> epoch: 1 batch: 700 loss: 0.18102 train_acc: 0.95 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 750 loss: 0.21231 train_acc: 0.91 dev_acc: 0.88\n",
      ">> epoch: 1 batch: 800 loss: 0.22898 train_acc: 0.92 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 850 loss: 0.35349 train_acc: 0.88 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 900 loss: 0.24418 train_acc: 0.92 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 950 loss: 0.20357 train_acc: 0.92 dev_acc: 0.85\n",
      ">> epoch: 1 batch: 1000 loss: 0.1399 train_acc: 0.95 dev_acc: 0.87\n",
      ">> epoch: 1 batch: 1050 loss: 0.18658 train_acc: 0.95 dev_acc: 0.87\n",
      ">> epoch: 1 batch: 1100 loss: 0.15499 train_acc: 0.92 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 1150 loss: 0.19556 train_acc: 0.93 dev_acc: 0.88\n",
      ">> epoch: 1 batch: 1200 loss: 0.19999 train_acc: 0.93 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 1250 loss: 0.22159 train_acc: 0.91 dev_acc: 0.89\n",
      ">> epoch: 1 batch: 1300 loss: 0.31393 train_acc: 0.92 dev_acc: 0.92\n",
      ">> epoch: 1 batch: 1350 loss: 0.12244 train_acc: 0.95 dev_acc: 0.93\n",
      ">> epoch: 1 batch: 1400 loss: 0.34917 train_acc: 0.9 dev_acc: 0.93\n",
      ">> epoch: 1 batch: 1450 loss: 0.12439 train_acc: 0.95 dev_acc: 0.93\n",
      ">> epoch: 1 batch: 1500 loss: 0.09658 train_acc: 0.96 dev_acc: 0.91\n",
      ">> epoch: 1 batch: 1550 loss: 0.20225 train_acc: 0.92 dev_acc: 0.88\n",
      ">> epoch: 1 batch: 1600 loss: 0.24856 train_acc: 0.94 dev_acc: 0.87\n",
      ">> epoch: 1 batch: 1650 loss: 0.13031 train_acc: 0.99 dev_acc: 0.87\n",
      ">> epoch: 1 batch: 1700 loss: 0.32088 train_acc: 0.87 dev_acc: 0.85\n",
      ">> epoch: 1 batch: 1750 loss: 0.14615 train_acc: 0.95 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 0 loss: 0.13136 train_acc: 0.97 dev_acc: 0.95\n",
      ">> epoch: 2 batch: 50 loss: 0.08358 train_acc: 0.97 dev_acc: 0.93\n",
      ">> epoch: 2 batch: 100 loss: 0.14851 train_acc: 0.94 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 150 loss: 0.03189 train_acc: 0.99 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 200 loss: 0.21749 train_acc: 0.95 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 250 loss: 0.15758 train_acc: 0.94 dev_acc: 0.88\n",
      ">> epoch: 2 batch: 300 loss: 0.18492 train_acc: 0.93 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 350 loss: 0.05801 train_acc: 0.98 dev_acc: 0.88\n",
      ">> epoch: 2 batch: 400 loss: 0.25226 train_acc: 0.95 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 450 loss: 0.15772 train_acc: 0.96 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 500 loss: 0.13425 train_acc: 0.93 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 550 loss: 0.21927 train_acc: 0.91 dev_acc: 0.93\n",
      ">> epoch: 2 batch: 600 loss: 0.02684 train_acc: 0.99 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 650 loss: 0.14044 train_acc: 0.94 dev_acc: 0.94\n",
      ">> epoch: 2 batch: 700 loss: 0.11854 train_acc: 0.94 dev_acc: 0.96\n",
      "-----saving model------\n",
      "-----saving model end------\n",
      ">> epoch: 2 batch: 750 loss: 0.09923 train_acc: 0.96 dev_acc: 0.97\n",
      "-----saving model------\n",
      "-----saving model end------\n",
      ">> epoch: 2 batch: 800 loss: 0.16859 train_acc: 0.93 dev_acc: 0.94\n",
      ">> epoch: 2 batch: 850 loss: 0.13303 train_acc: 0.95 dev_acc: 0.97\n",
      ">> epoch: 2 batch: 900 loss: 0.03222 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 950 loss: 0.17922 train_acc: 0.91 dev_acc: 0.93\n",
      ">> epoch: 2 batch: 1000 loss: 0.182 train_acc: 0.94 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 1050 loss: 0.12874 train_acc: 0.96 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1100 loss: 0.18675 train_acc: 0.94 dev_acc: 0.94\n",
      ">> epoch: 2 batch: 1150 loss: 0.20798 train_acc: 0.92 dev_acc: 0.89\n",
      ">> epoch: 2 batch: 1200 loss: 0.05562 train_acc: 0.98 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 1250 loss: 0.12079 train_acc: 0.96 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1300 loss: 0.10801 train_acc: 0.96 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1350 loss: 0.08766 train_acc: 0.97 dev_acc: 0.93\n",
      ">> epoch: 2 batch: 1400 loss: 0.12706 train_acc: 0.97 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1450 loss: 0.08608 train_acc: 0.96 dev_acc: 0.92\n",
      ">> epoch: 2 batch: 1500 loss: 0.16268 train_acc: 0.92 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1550 loss: 0.17474 train_acc: 0.93 dev_acc: 0.89\n",
      ">> epoch: 2 batch: 1600 loss: 0.05035 train_acc: 0.99 dev_acc: 0.91\n",
      ">> epoch: 2 batch: 1650 loss: 0.17217 train_acc: 0.94 dev_acc: 0.88\n",
      ">> epoch: 2 batch: 1700 loss: 0.04683 train_acc: 0.99 dev_acc: 0.88\n",
      ">> epoch: 2 batch: 1750 loss: 0.0982 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 0 loss: 0.04715 train_acc: 0.99 dev_acc: 0.89\n",
      ">> epoch: 3 batch: 50 loss: 0.06942 train_acc: 0.97 dev_acc: 0.88\n",
      ">> epoch: 3 batch: 100 loss: 0.11386 train_acc: 0.96 dev_acc: 0.94\n",
      ">> epoch: 3 batch: 150 loss: 0.06024 train_acc: 0.98 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 200 loss: 0.10091 train_acc: 0.95 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 250 loss: 0.03495 train_acc: 1.0 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 300 loss: 0.0083 train_acc: 1.0 dev_acc: 0.96\n",
      ">> epoch: 3 batch: 350 loss: 0.07537 train_acc: 0.97 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 400 loss: 0.15554 train_acc: 0.92 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 450 loss: 0.08436 train_acc: 0.97 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 500 loss: 0.06585 train_acc: 0.98 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 550 loss: 0.08132 train_acc: 0.98 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 600 loss: 0.06164 train_acc: 0.98 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 650 loss: 0.06783 train_acc: 0.97 dev_acc: 0.89\n",
      ">> epoch: 3 batch: 700 loss: 0.15189 train_acc: 0.94 dev_acc: 0.92\n",
      ">> epoch: 3 batch: 750 loss: 0.04407 train_acc: 0.99 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 800 loss: 0.10536 train_acc: 0.98 dev_acc: 0.88\n",
      ">> epoch: 3 batch: 850 loss: 0.13978 train_acc: 0.94 dev_acc: 0.89\n",
      ">> epoch: 3 batch: 900 loss: 0.07475 train_acc: 0.98 dev_acc: 0.89\n",
      ">> epoch: 3 batch: 950 loss: 0.16018 train_acc: 0.95 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 1000 loss: 0.13162 train_acc: 0.95 dev_acc: 0.88\n",
      ">> epoch: 3 batch: 1050 loss: 0.12767 train_acc: 0.96 dev_acc: 0.88\n",
      ">> epoch: 3 batch: 1100 loss: 0.05246 train_acc: 0.99 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 1150 loss: 0.05487 train_acc: 0.98 dev_acc: 0.92\n",
      ">> epoch: 3 batch: 1200 loss: 0.08379 train_acc: 0.95 dev_acc: 0.89\n",
      ">> epoch: 3 batch: 1250 loss: 0.06864 train_acc: 0.97 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 1300 loss: 0.15161 train_acc: 0.95 dev_acc: 0.9\n",
      ">> epoch: 3 batch: 1350 loss: 0.16791 train_acc: 0.93 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 1400 loss: 0.17649 train_acc: 0.93 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 1450 loss: 0.11105 train_acc: 0.94 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 1500 loss: 0.05275 train_acc: 0.99 dev_acc: 0.91\n",
      ">> epoch: 3 batch: 1550 loss: 0.21302 train_acc: 0.93 dev_acc: 0.94\n",
      ">> epoch: 3 batch: 1600 loss: 0.20808 train_acc: 0.95 dev_acc: 0.94\n",
      ">> epoch: 3 batch: 1650 loss: 0.13207 train_acc: 0.97 dev_acc: 0.93\n",
      ">> epoch: 3 batch: 1700 loss: 0.0431 train_acc: 0.99 dev_acc: 0.94\n",
      ">> epoch: 3 batch: 1750 loss: 0.09975 train_acc: 0.96 dev_acc: 0.94\n",
      ">> epoch: 4 batch: 0 loss: 0.03966 train_acc: 0.99 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 50 loss: 0.02953 train_acc: 0.99 dev_acc: 0.89\n",
      ">> epoch: 4 batch: 100 loss: 0.02245 train_acc: 0.99 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 150 loss: 0.07468 train_acc: 0.96 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 200 loss: 0.04951 train_acc: 0.97 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 250 loss: 0.11647 train_acc: 0.96 dev_acc: 0.91\n",
      ">> epoch: 4 batch: 300 loss: 0.07857 train_acc: 0.97 dev_acc: 0.9\n",
      ">> epoch: 4 batch: 350 loss: 0.00592 train_acc: 1.0 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 400 loss: 0.03946 train_acc: 0.99 dev_acc: 0.89\n",
      ">> epoch: 4 batch: 450 loss: 0.02068 train_acc: 0.99 dev_acc: 0.95\n",
      ">> epoch: 4 batch: 500 loss: 0.06965 train_acc: 0.95 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 550 loss: 0.04153 train_acc: 0.99 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 600 loss: 0.03753 train_acc: 0.98 dev_acc: 0.91\n",
      ">> epoch: 4 batch: 650 loss: 0.05521 train_acc: 0.97 dev_acc: 0.89\n",
      ">> epoch: 4 batch: 700 loss: 0.10123 train_acc: 0.97 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 750 loss: 0.00923 train_acc: 1.0 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 800 loss: 0.12368 train_acc: 0.96 dev_acc: 0.9\n",
      ">> epoch: 4 batch: 850 loss: 0.08608 train_acc: 0.98 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 900 loss: 0.08313 train_acc: 0.98 dev_acc: 0.93\n",
      ">> epoch: 4 batch: 950 loss: 0.06978 train_acc: 0.97 dev_acc: 0.88\n",
      ">> epoch: 4 batch: 1000 loss: 0.20459 train_acc: 0.94 dev_acc: 0.94\n",
      ">> epoch: 4 batch: 1050 loss: 0.14303 train_acc: 0.96 dev_acc: 0.9\n",
      ">> epoch: 4 batch: 1100 loss: 0.11871 train_acc: 0.96 dev_acc: 0.88\n",
      ">> epoch: 4 batch: 1150 loss: 0.09744 train_acc: 0.95 dev_acc: 0.95\n",
      ">> epoch: 4 batch: 1200 loss: 0.05516 train_acc: 0.99 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 1250 loss: 0.07888 train_acc: 0.97 dev_acc: 0.92\n",
      ">> epoch: 4 batch: 1300 loss: 0.05369 train_acc: 0.99 dev_acc: 0.91\n",
      ">> epoch: 4 batch: 1350 loss: 0.05637 train_acc: 0.99 dev_acc: 0.9\n",
      ">> epoch: 4 batch: 1400 loss: 0.08487 train_acc: 0.95 dev_acc: 0.95\n",
      ">> epoch: 4 batch: 1450 loss: 0.08041 train_acc: 0.96 dev_acc: 0.95\n",
      ">> epoch: 4 batch: 1500 loss: 0.10331 train_acc: 0.96 dev_acc: 0.94\n",
      ">> epoch: 4 batch: 1550 loss: 0.02771 train_acc: 0.99 dev_acc: 0.94\n",
      ">> epoch: 4 batch: 1600 loss: 0.03853 train_acc: 0.99 dev_acc: 0.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, target)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 每50个batch做一次\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\py39torch\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ProgramData\\Anaconda3\\envs\\py39torch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id2label, _ = get_label()\n",
    "\n",
    "train_dataset = Dataset('train')\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "dev_dataset = Dataset('dev')\n",
    "dev_loader = data.DataLoader(dev_dataset, batch_size=100)\n",
    "\n",
    "model = TextCNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "dev_acc_list = []\n",
    "dev_acc = 0\n",
    "for e in range(10):\n",
    "    for b, (input, mask, target) in enumerate(train_loader):\n",
    "        input = input.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        pred = model(input, mask)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每50个batch做一次\n",
    "        if b % 50 != 0:\n",
    "            continue\n",
    "        # torch.argmax dim = 1 返回每一行中最大值的索引\n",
    "        y_pred = torch.argmax(pred, dim=1)\n",
    "        # pred, true, target_names=None, output_dict=False\n",
    "        report =evaluate(y_pred.cpu().data.numpy(), target.cpu().data.numpy(), output_dict=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dev_input, dev_mask, dev_target = iter(dev_loader).__next__()\n",
    "            dev_input = dev_input.to(DEVICE)\n",
    "            dev_mask = dev_mask.to(DEVICE)\n",
    "            dev_target = dev_target.to(DEVICE)\n",
    "            dev_pred = model(dev_input, dev_mask)\n",
    "            dev_pred_ = torch.argmax(dev_pred, dim=1)\n",
    "            dev_report = evaluate(dev_pred_.cpu().data.numpy(), dev_target.cpu().data.numpy(), output_dict=True)\n",
    "        print(\n",
    "            '>> epoch:', e,\n",
    "            'batch:', b,\n",
    "            'loss:', round(loss.item(), 5),\n",
    "            'train_acc:', report['accuracy'],\n",
    "            'dev_acc:', dev_report['accuracy'],\n",
    "            # type(dev_report['accuracy'])\n",
    "            # type(dev_report['accuracy'])\n",
    "        )\n",
    "        if dev_acc < dev_report['accuracy'] and dev_report['accuracy']>0.95:\n",
    "            print(\"-----saving model------\")\n",
    "            # torch.save(model, MODEL_DIR + f'{e}.pth')\n",
    "            torch.save(model, MODEL_DIR + 'the_best_95.pth')\n",
    "            print(\"-----saving model end------\")\n",
    "            dev_acc = dev_report['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
